{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/fsem/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nlinec import get_positive_data, get_all_types, get_granularity, construct_hypothesis, get_type, combine_premise_hypothesis, get_models_dir, get_negative_data, combine_positive_negative_data\n",
    "from nlinec.predict import predict_type\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRANULARITY = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading augmented_train.json: 793487it [00:12, 64709.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading negative data from /home/psaegert/Projects/nli-nec/src/nlinec/../../data/derived/negative_data/augmented_train.json_42.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading g_dev.json: 2202it [00:00, 216870.35it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_data = get_positive_data(\"augmented_train.json\", explode=True)\n",
    "negative_data = get_negative_data(\"augmented_train.json\", random_state=42)\n",
    "dev_data = get_positive_data(\"g_dev.json\", explode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_positive_negative_data(positive_data, negative_data, frac=0.5, random_state=42)\n",
    "del positive_data, negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_type</th>\n",
       "      <th>mention_span</th>\n",
       "      <th>sentence</th>\n",
       "      <th>granularity</th>\n",
       "      <th>label</th>\n",
       "      <th>type_2</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/location/country</td>\n",
       "      <td>We</td>\n",
       "      <td>We did not do anything at that time.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>country</td>\n",
       "      <td>We is a country.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/organization/company</td>\n",
       "      <td>antibody</td>\n",
       "      <td>`` We don't know the effect of our antibody on...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>company</td>\n",
       "      <td>antibody is a company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/other/product</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>The Visigoths of Spain were defeated when the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>product</td>\n",
       "      <td>Lisbon is a product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/other/product</td>\n",
       "      <td>non food crops or inedible waste products</td>\n",
       "      <td>Cellulosic ethanol production uses non food cr...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>product</td>\n",
       "      <td>non food crops or inedible waste products is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/location/geography/body_of_water</td>\n",
       "      <td>traditional games</td>\n",
       "      <td>In caffeehouses around you could see people sm...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>geography</td>\n",
       "      <td>traditional games is a geography.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864998</th>\n",
       "      <td>/person/artist</td>\n",
       "      <td>transfer</td>\n",
       "      <td>It marked the first peaceful transfer of power...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>artist</td>\n",
       "      <td>transfer is a artist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864999</th>\n",
       "      <td>/other/internet</td>\n",
       "      <td>American</td>\n",
       "      <td>Right now, the American populace is spending a...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>internet</td>\n",
       "      <td>American is a internet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865001</th>\n",
       "      <td>/other/event/holiday</td>\n",
       "      <td>American</td>\n",
       "      <td>Right now, the American populace is spending a...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>event</td>\n",
       "      <td>American is a event.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865002</th>\n",
       "      <td>/location/structure</td>\n",
       "      <td>American</td>\n",
       "      <td>Right now, the American populace is spending a...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>structure</td>\n",
       "      <td>American is a structure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865006</th>\n",
       "      <td>/other/health</td>\n",
       "      <td>Japanese martial arts classes such as Aikido ,...</td>\n",
       "      <td>Sensei is often used to address the teacher in...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>health</td>\n",
       "      <td>Japanese martial arts classes such as Aikido ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813973 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 full_type  \\\n",
       "2                        /location/country   \n",
       "5                    /organization/company   \n",
       "6                           /other/product   \n",
       "8                           /other/product   \n",
       "10       /location/geography/body_of_water   \n",
       "...                                    ...   \n",
       "1864998                     /person/artist   \n",
       "1864999                    /other/internet   \n",
       "1865001               /other/event/holiday   \n",
       "1865002                /location/structure   \n",
       "1865006                      /other/health   \n",
       "\n",
       "                                              mention_span  \\\n",
       "2                                                       We   \n",
       "5                                                 antibody   \n",
       "6                                                   Lisbon   \n",
       "8                non food crops or inedible waste products   \n",
       "10                                       traditional games   \n",
       "...                                                    ...   \n",
       "1864998                                           transfer   \n",
       "1864999                                           American   \n",
       "1865001                                           American   \n",
       "1865002                                           American   \n",
       "1865006  Japanese martial arts classes such as Aikido ,...   \n",
       "\n",
       "                                                  sentence  granularity  \\\n",
       "2                     We did not do anything at that time.            2   \n",
       "5        `` We don't know the effect of our antibody on...            2   \n",
       "6        The Visigoths of Spain were defeated when the ...            2   \n",
       "8        Cellulosic ethanol production uses non food cr...            2   \n",
       "10       In caffeehouses around you could see people sm...            3   \n",
       "...                                                    ...          ...   \n",
       "1864998  It marked the first peaceful transfer of power...            2   \n",
       "1864999  Right now, the American populace is spending a...            2   \n",
       "1865001  Right now, the American populace is spending a...            3   \n",
       "1865002  Right now, the American populace is spending a...            2   \n",
       "1865006  Sensei is often used to address the teacher in...            2   \n",
       "\n",
       "         label     type_2                                         hypothesis  \n",
       "2            2    country                                   We is a country.  \n",
       "5            1    company                             antibody is a company.  \n",
       "6            1    product                               Lisbon is a product.  \n",
       "8            2    product  non food crops or inedible waste products is a...  \n",
       "10           1  geography                  traditional games is a geography.  \n",
       "...        ...        ...                                                ...  \n",
       "1864998      1     artist                              transfer is a artist.  \n",
       "1864999      1   internet                            American is a internet.  \n",
       "1865001      1      event                               American is a event.  \n",
       "1865002      2  structure                           American is a structure.  \n",
       "1865006      1     health  Japanese martial arts classes such as Aikido ,...  \n",
       "\n",
       "[813973 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the basic type\n",
    "data[f'type_{GRANULARITY}'] = data['full_type'].apply(lambda x: get_type(x, GRANULARITY))\n",
    "dev_data[f'type_{GRANULARITY}'] = dev_data['full_type'].apply(lambda x: get_type(x, GRANULARITY))\n",
    "\n",
    "# Remove the rows with type None or \"other\"\n",
    "data = data[data[f'type_{GRANULARITY}'].notna()]\n",
    "dev_data = dev_data[dev_data[f'type_{GRANULARITY}'].notna()]\n",
    "# data = data[data[f'type_{GRANULARITY}'] != 'other']\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates(subset=['mention_span', 'sentence', f'type_{GRANULARITY}'])\n",
    "dev_data = dev_data.drop_duplicates(subset=['mention_span', 'sentence', f'type_{GRANULARITY}'])\n",
    "\n",
    "# Construct the hypothesis\n",
    "data[\"hypothesis\"] = data.apply(lambda row: construct_hypothesis(row[\"mention_span\"], row[f'type_{GRANULARITY}']), axis=1)\n",
    "dev_data[\"hypothesis\"] = dev_data.apply(lambda row: construct_hypothesis(row[\"mention_span\"], row[f'type_{GRANULARITY}']), axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gran_types = []\n",
    "for i in [1, 2, 3]:\n",
    "    all_types = get_all_types(granularity=i)\n",
    "    all_types['granularity'] = all_types['full_type'].apply(lambda x: get_granularity(x))\n",
    "    gran_types.append(all_types[all_types['granularity'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlinec import get_models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Make the data usable by the model\n",
    "# The input is of the form: sentence</s><s>hypothesis\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train_dataset = Dataset.from_pandas(data.loc[:, [\"sentence\", \"hypothesis\", \"label\"]])\n",
    "dev_dataset = Dataset.from_pandas(dev_data.loc[:, [\"sentence\", \"hypothesis\", \"label\"]])\n",
    "\n",
    "def tokenize_function(examples: dict) -> dict:\n",
    "    # input_text = examples[\"sentence\"] + \"</s><s>\" + examples[\"hypothesis\"]\n",
    "    input_text = [combine_premise_hypothesis(sentence, hypothesis) for sentence, hypothesis in zip(examples[\"sentence\"], examples[\"hypothesis\"])]\n",
    "    return tokenizer(input_text, max_length=model.config.max_position_embeddings, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dev_dataset = dev_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(get_models_dir(), f'nlinec-{GRANULARITY}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        predictions = predict_type(model, tokenizer, list(dev_data['sentence']), list(dev_data['mention_span']), list(gran_types[1]['type']), return_str=True, verbose=True)\n",
    "    print((dev_data[f'type_{GRANULARITY}'] == predictions).mean())\n",
    "    return {\"accuracy\": (dev_data[f'type_{GRANULARITY}'] == predictions).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=3000,\n",
    "    # load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # evaluation_strategy='no',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, sentence. If hypothesis, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/psaegert/miniconda3/envs/fsem/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 813973\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 25436\n",
      "  Number of trainable parameters = 355362819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='25436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/25436 00:01 < 9:35:19, 0.74 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, __index_level_0__, sentence. If hypothesis, __index_level_0__, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 866\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 866\n",
      "  Batch size = 8\n",
      "Predicting types: 100%|█████████▉| 865/866 [00:48<00:00, 21.76it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/psaegert/Projects/nli-nec/src/nlinec/../../models/nlinec-positive-2\n",
      "Configuration saved in /home/psaegert/Projects/nli-nec/src/nlinec/../../models/nlinec-positive-2/config.json\n",
      "Model weights saved in /home/psaegert/Projects/nli-nec/src/nlinec/../../models/nlinec-positive-2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting types: 100%|██████████| 866/866 [00:50<00:00, 17.16it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dev_predictions = predict_type(model, tokenizer, list(dev_data['sentence']), list(dev_data['mention_span']), list(gran_types[1]['type']), return_str=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mention_span</th>\n",
       "      <th>full_type</th>\n",
       "      <th>sentence</th>\n",
       "      <th>granularity</th>\n",
       "      <th>label</th>\n",
       "      <th>type_2</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>prediction_before</th>\n",
       "      <th>prediction_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japan</td>\n",
       "      <td>/location/country</td>\n",
       "      <td>Japan's wholesale prices in September rose 3.3...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>country</td>\n",
       "      <td>Japan is a country.</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the Bank of Japan</td>\n",
       "      <td>/location/structure</td>\n",
       "      <td>Japan's wholesale prices in September rose 3.3...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>structure</td>\n",
       "      <td>the Bank of Japan is a structure.</td>\n",
       "      <td>title</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the Bank of Japan</td>\n",
       "      <td>/organization/government</td>\n",
       "      <td>Japan's wholesale prices in September rose 3.3...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>government</td>\n",
       "      <td>the Bank of Japan is a government.</td>\n",
       "      <td>title</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the Bank</td>\n",
       "      <td>/location/structure</td>\n",
       "      <td>Japan's wholesale prices in September rose 3.3...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>structure</td>\n",
       "      <td>the Bank is a structure.</td>\n",
       "      <td>event</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Japan 's</td>\n",
       "      <td>/location/country</td>\n",
       "      <td>Japan 's wholesale prices in September rose 3...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>country</td>\n",
       "      <td>Japan 's is a country.</td>\n",
       "      <td>product</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>Europe</td>\n",
       "      <td>/location/geography</td>\n",
       "      <td>There were no major Eurobond or foreign bond o...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>geography</td>\n",
       "      <td>Europe is a geography.</td>\n",
       "      <td>title</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>$ 150 million</td>\n",
       "      <td>/other/currency</td>\n",
       "      <td>$ 150 million of 9 % debentures due Oct. 15, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>currency</td>\n",
       "      <td>$ 150 million is a currency.</td>\n",
       "      <td>product</td>\n",
       "      <td>currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>the Treasury 's</td>\n",
       "      <td>/organization/government</td>\n",
       "      <td>The non-callable issue, which can be put back ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>government</td>\n",
       "      <td>the Treasury 's is a government.</td>\n",
       "      <td>product</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>the Treasury 's</td>\n",
       "      <td>/organization/government</td>\n",
       "      <td>The issue, which is puttable back to the compa...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>government</td>\n",
       "      <td>the Treasury 's is a government.</td>\n",
       "      <td>product</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>$ 200 million</td>\n",
       "      <td>/other/currency</td>\n",
       "      <td>$ 200 million of stripped mortgage securities...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>currency</td>\n",
       "      <td>$ 200 million is a currency.</td>\n",
       "      <td>product</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>866 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           mention_span                 full_type  \\\n",
       "3                 Japan         /location/country   \n",
       "5     the Bank of Japan       /location/structure   \n",
       "7     the Bank of Japan  /organization/government   \n",
       "15             the Bank       /location/structure   \n",
       "18             Japan 's         /location/country   \n",
       "...                 ...                       ...   \n",
       "3264             Europe       /location/geography   \n",
       "3267      $ 150 million           /other/currency   \n",
       "3275    the Treasury 's  /organization/government   \n",
       "3278    the Treasury 's  /organization/government   \n",
       "3281      $ 200 million           /other/currency   \n",
       "\n",
       "                                               sentence  granularity  label  \\\n",
       "3     Japan's wholesale prices in September rose 3.3...            2      2   \n",
       "5     Japan's wholesale prices in September rose 3.3...            2      2   \n",
       "7     Japan's wholesale prices in September rose 3.3...            2      2   \n",
       "15    Japan's wholesale prices in September rose 3.3...            2      2   \n",
       "18     Japan 's wholesale prices in September rose 3...            2      2   \n",
       "...                                                 ...          ...    ...   \n",
       "3264  There were no major Eurobond or foreign bond o...            2      2   \n",
       "3267   $ 150 million of 9 % debentures due Oct. 15, ...            2      2   \n",
       "3275  The non-callable issue, which can be put back ...            2      2   \n",
       "3278  The issue, which is puttable back to the compa...            2      2   \n",
       "3281   $ 200 million of stripped mortgage securities...            2      2   \n",
       "\n",
       "          type_2                          hypothesis prediction_before  \\\n",
       "3        country                 Japan is a country.           country   \n",
       "5      structure   the Bank of Japan is a structure.             title   \n",
       "7     government  the Bank of Japan is a government.             title   \n",
       "15     structure            the Bank is a structure.             event   \n",
       "18       country              Japan 's is a country.           product   \n",
       "...          ...                                 ...               ...   \n",
       "3264   geography              Europe is a geography.             title   \n",
       "3267    currency        $ 150 million is a currency.           product   \n",
       "3275  government    the Treasury 's is a government.           product   \n",
       "3278  government    the Treasury 's is a government.           product   \n",
       "3281    currency        $ 200 million is a currency.           product   \n",
       "\n",
       "     prediction_after  \n",
       "3             country  \n",
       "5             company  \n",
       "7             company  \n",
       "15            company  \n",
       "18            country  \n",
       "...               ...  \n",
       "3264          country  \n",
       "3267         currency  \n",
       "3275          company  \n",
       "3278       government  \n",
       "3281          product  \n",
       "\n",
       "[866 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data['prediction_after'] = dev_predictions\n",
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample accuracy after training: 0.5958429561200924\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample accuracy after training: {(dev_data[f'type_{GRANULARITY}'] == dev_data['prediction_after']).mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
